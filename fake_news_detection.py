# -*- coding: utf-8 -*-
"""Fake_news_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-ao3PkraJX-lA6aa7PlSgc8HRBvz_SJh
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
fake_news = pd.read_csv('/content/drive/MyDrive/Fake.csv')
# !pip install --upgrade plotly

true_news=pd.read_csv('/content/drive/MyDrive/True.csv')

"""shape of fake and real dataset with rows and columns and information"""

print ("The shape of the  data is (row, column):"+ str(fake_news.shape))
print (fake_news.info())
print("\n --------------------------------------- \n")
print ("The shape of the  data is (row, column):"+ str(true_news.shape))
print (true_news.info())

"""handling missing data"""

# Check for missing values in the entire DataFrame
missing_values = fake_news.isnull().sum()
missing_values_true = true_news.isnull().sum()
# Count the total number of missing values in the DataFrame
total_missing_values = fake_news.isnull().sum().sum()
total_missing_values_true = true_news.isnull().sum().sum()
# Print the results
print("Missing false values in the entire DataFrame:")
print(missing_values)
print("\nMissing true values in the entire DataFrame:")
print(missing_values_true)
print("\nTotal number of missing false values:", total_missing_values)
print("\nTotal number of missing true values:", total_missing_values_true)

"""Preprocessing and cleaning"""

#Target variable for fake news
fake_news['output']=0
#Target variable for true news
true_news['output']=1

"""concatenating title and text"""

#Concatenating and dropping for fake news
fake_news['news']=fake_news['title']+fake_news['text']
fake_news=fake_news.drop(['title', 'text'], axis=1)

#Concatenating and dropping for true news
true_news['news']=true_news['title']+true_news['text']
true_news=true_news.drop(['title', 'text'], axis=1)

#Rearranging the columns
fake_news = fake_news[['subject', 'date', 'news','output']]
true_news = true_news[['subject', 'date', 'news','output']]

"""Converting the date columns to datetime format"""

fake_news['date'].value_counts()
true_news['date'].value_counts()

#Removing links and the headline from the date column
fake_news=fake_news[~fake_news.date.str.contains("http")]
fake_news=fake_news[~fake_news.date.str.contains("HOST")]
fake_news

"""appending two datasets"""

frames = [fake_news, true_news]
news_dataset = pd.concat(frames)
news_dataset

"""Text processing

News-Punctuation Cleaning
"""

#Creating a copy 
clean_news=news_dataset.copy()

import re
import string
def review_cleaning(text):
    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation
    and remove words containing numbers.'''
    text = str(text).lower()
    text = re.sub('\[.*?\]', '', text)
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    return text
clean_news['news']=clean_news['news'].apply(lambda x:review_cleaning(x))
clean_news.head()

"""News-Stop words"""

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from wordcloud import WordCloud,STOPWORDS
stop = stopwords.words('english')
clean_news['news'] = clean_news['news'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
clean_news.head()

"""Count of news subject"""

import seaborn as sns
#Plotting the frequency plot
ax = sns.countplot(x="subject", data=clean_news,
                   facecolor=(0, 0, 0, 0),
                   linewidth=3,
                   edgecolor=sns.color_palette("dark", 2))

#Setting labels and font size
ax.set(xlabel='Type of news', ylabel='Number of news',title='Count of news type')
ax.xaxis.get_label().set_fontsize(12)
ax.yaxis.get_label().set_fontsize(12)

"""Count of news subject based on true or fake"""

g = sns.catplot(x="subject", col="output",
                data=clean_news, kind="count",
                height=4, aspect=2)

#Rotating the xlabels
g.set_xticklabels(rotation=45)

"""Count of fake news and true news"""

ax=sns.countplot(x="output", data=clean_news)

#Setting labels and font size
ax.set(xlabel='Output', ylabel='Count of fake/true',title='Count of fake and true news')
ax.xaxis.get_label().set_fontsize(15)
ax.yaxis.get_label().set_fontsize(15)

"""Deriving new features from the news suchas word count,measures the sentiment,length of news"""

from textblob import TextBlob
from plotly import tools
import plotly.graph_objs as go
import matplotlib.pyplot as plt 
#Extracting the features from the news
clean_news['polarity'] = clean_news['news'].map(lambda text: TextBlob(text).sentiment.polarity)
clean_news['review_len'] = clean_news['news'].astype(str).apply(len)
clean_news['word_count'] = clean_news['news'].apply(lambda x: len(str(x).split()))

#Plotting the distribution of the extracted feature
plt.figure(figsize = (20, 5))
plt.style.use('seaborn-white')
plt.subplot(131)
sns.distplot(clean_news['polarity'])
fig = plt.gcf()
plt.subplot(132)
sns.distplot(clean_news['review_len'])
fig = plt.gcf()
plt.subplot(133)
sns.distplot(clean_news['word_count'])
fig = plt.gcf()

"""Bi gram analysis"""

corpus=clean_news['news']

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from plotly.offline import iplot

# Create a CountVectorizer object with desired Bi-grams range
ngram_vectorizer = CountVectorizer(ngram_range=(2, 2))

# Fit the vectorizer on the corpus
ngram_vectorizer.fit(corpus)

# Transform the corpus into a sparse matrix of Bi-grams counts
X = ngram_vectorizer.transform(corpus)

# Get the top 10 Bi-grams and their counts
sum_words = X.sum(axis=0)
words_freq = [(word, sum_words[0, idx]) for word, idx in ngram_vectorizer.vocabulary_.items()]
words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)[:10]

# Print the top 10 Bi-grams and their counts
for word, freq in words_freq:
    print(word, freq)
# Create a bar chart of the top 10 Bi-grams
fig = go.Figure(go.Bar(
    x=[word for word, freq in words_freq],
    y=[freq for word, freq in words_freq]
))
fig.update_layout(title="Top 10 Bi-grams", xaxis_title="Bi-grams", yaxis_title="Frequency")
iplot(fig)

"""Trigram analysis"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer

# Create a CountVectorizer object with desired Tri-grams range
ngram_vectorizer = CountVectorizer(ngram_range=(3,3))

# Fit the vectorizer on the corpus
ngram_vectorizer.fit(corpus)

# Transform the corpus into a sparse matrix of Trigrams counts
X = ngram_vectorizer.transform(corpus)

# Get the top 10 Trigrams and their counts
sum_words = X.sum(axis=0)
words_freq = [(word, sum_words[0, idx]) for word, idx in ngram_vectorizer.vocabulary_.items()]
words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)[:10]

# Print the top 10 Trigrams and their counts
for word, freq in words_freq:
    print(word, freq)
# Create a bar chart of the top 10 Trigrams
fig = go.Figure(go.Bar(
    x=[word for word, freq in words_freq],
    y=[freq for word, freq in words_freq]
))
fig.update_layout(title="Top 10 Tri-grams", xaxis_title="Tri-grams", yaxis_title="Frequency")
iplot(fig)

"""word cloud for fake & real"""

text = fake_news["news"]
wordcloud = WordCloud(
    width = 3000,
    height = 2000,
    background_color = 'black',
    stopwords = STOPWORDS).generate(str(text))
fig = plt.figure(
    figsize = (9,9),
    facecolor = 'k',
    edgecolor = 'k')
plt.imshow(wordcloud, interpolation = 'bilinear')
plt.axis('off')
plt.tight_layout(pad=0)
plt.show()

"""Stemming"""

#Extracting 'reviews' for processing
news_features=clean_news.copy()
news_features=news_features[['news']].reset_index(drop=True)
news_features.head()

from nltk.stem.porter import PorterStemmer
stop_words = set(stopwords.words("english"))
#Performing stemming on the review dataframe
ps = PorterStemmer()

#splitting and adding the stemmed words except stopwords
corpus = []
for i in range(0, len(news_features)):
    news = re.sub('[^a-zA-Z]', ' ', news_features['news'][i])
    news= news.lower()
    news = news.split()
    news = [ps.stem(word) for word in news if not word in stop_words]
    news = ' '.join(news)
    corpus.append(news)

corpus[1]

"""vectorizer"""

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=5000,ngram_range=(2,2))
# TF-IDF feature matrix
X= tfidf_vectorizer.fit_transform(news_features['news'])
X.shape

y=clean_news['output']

"""Checking for balance of data"""

from collections import Counter
print(f'Original dataset shape : {Counter(y)}')

"""train and test the splitted data - 75: 25"""

## Divide the dataset into Train and Test
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler(with_mean=False)
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""Model  selection"""

from sklearn.model_selection import cross_val_score
import sklearn
logreg_cv = LogisticRegression(C=0.1, penalty='l2', solver='lbfgs', max_iter=1000, random_state=42)
dt_cv=DecisionTreeClassifier(criterion='gini', max_depth=5, min_samples_split=2,min_samples_leaf=1, max_features=None, random_state=42)
cv_dict = {0: 'Logistic Regression', 1: 'Decision Tree'}
cv_models=[logreg_cv,dt_cv]

#Printing the accuracy
for i,model_ld in enumerate(cv_models):
    print("{} Test Accuracy: {}".format(cv_dict[i],cross_val_score(model_ld, X, y, cv=10, scoring ='accuracy').mean()))

"""undersampling & over sampling decision tree,logistic regression"""

from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier 
from sklearn.metrics import accuracy_score
# Perform oversampling of minority class
oversampler = RandomOverSampler(sampling_strategy='minority')
X_train_oversampled, y_train_oversampled = oversampler.fit_resample(X_train, y_train)

# Perform undersampling of majority class
undersampler = RandomUnderSampler(sampling_strategy='majority')
X_train_oversampled_undersampled, y_train_oversampled_undersampled = undersampler.fit_resample(
    X_train_oversampled, y_train_oversampled)

# Train a decision tree classifier on the balanced data
dt_cv.fit(X_train_oversampled_undersampled, y_train_oversampled_undersampled)
logreg_cv.fit(X_train_oversampled_undersampled, y_train_oversampled_undersampled)

# Make predictions on the test set
y_pred_tree = dt_cv.predict(X_test)
y_pred_logreg = logreg_cv.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_tree)
print("Test Accuracy for decision tree:", accuracy)
accuracy = accuracy_score(y_test, y_pred_logreg)
print("Test Accuracy for logistic regression:", accuracy)

"""From the results, logistic regression outdone Decision Tree

cross validation with 5 folds for logistic regression
"""

import numpy as np
from sklearn.model_selection import cross_val_score, KFold
# Define the K-fold cross-validation scheme
kf = KFold(n_splits=5, shuffle=True, random_state=42)
# Train and evaluate the model using cross-validation
cv_scores = cross_val_score(logreg_cv, X, y, cv=kf)

# Print the cross-validation scores
print("Cross-validation scores: {}".format(cv_scores))
print("Mean cross-validation score: {}".format(np.mean(cv_scores)))

"""Logistic Regression with Hyperparameter Tuning"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score

# Define the parameter grid
param_grid = {'C': [0.01, 0.1],
              'penalty': ['l2', 'none'],
              'solver': ['newton-cg', 'lbfgs', 'sag']}
              
# Create a GridSearchCV object
grid_search = GridSearchCV(logreg_cv, param_grid, cv=5, n_jobs=-1)

# Fit the model on the training set
grid_search.fit(X_train, y_train)

# Print the best hyperparameters
print('Best Hyperparameters:', grid_search.best_params_)

# Predict on the testing set using the best model
y_pred = grid_search.predict(X_test)

# Calculate the accuracy score
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)

"""Fitting and predicting the model"""

logreg_cv.fit(X_train, y_train)
test_accuracy = logreg_cv.score(X_train, y_train)
print('Accuracy of logistic regression classifier on training set: {:.2f}'.format(test_accuracy))
y_pred = logreg_cv.predict(X_test)

logreg_cv.fit(X_test, y_test)
# evaluate fine-tuned model on test set
test_accuracy = logreg_cv.score(X_test, y_test)
print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(test_accuracy))

"""Evaluate the performance of Logistic regression model"""

import numpy as np
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Purples):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    thresh = cm.max() / 2.
    for i in range (cm.shape[0]):
      for j in range (cm.shape[1]):
            plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

from sklearn.metrics import classification_report, confusion_matrix
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score
from sklearn.metrics import accuracy_score
y_pred = y_pred.astype(int)

print("Accuracy: ", accuracy_score(y_test, y_pred))
print("Precision: ", precision_score(y_test, y_pred, average='weighted'))
print("Recall: ", recall_score(y_test, y_pred, average='weighted'))
print("F1 score: ", f1_score(y_test, y_pred, average='weighted'))
print('Classification Report:\n', classification_report(y_test, y_pred))
cm = confusion_matrix(y_test, y_pred)
print('Confusion Matrix:\n', plot_confusion_matrix(cm, classes=['Fake','True']))

"""ROC-AUC Curve of logistic regression"""

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve

logit_roc_auc = roc_auc_score(y_test, logreg_cv.predict(X_test))
fpr, tpr, thresholds = roc_curve(y_test, logreg_cv.predict_proba(X_test)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([-0.01, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.show()

"""cross validation with 5 folds for decision trees"""

# Define the K-fold cross-validation scheme
kf = KFold(n_splits=5, shuffle=True, random_state=42)
# Train and evaluate the model using cross-validation
cv_scores = cross_val_score(dt_cv, X, y, cv=kf)

# Print the cross-validation scores
print("Cross-validation scores: {}".format(cv_scores))
print("Mean cross-validation score: {}".format(np.mean(cv_scores)))

"""Decision Tree with Hyperparameter Tuning"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score

# Define the parameter grid
param_grid = {'criterion': ['gini', 'entropy'],
              'max_depth': [4, 5],
              'min_samples_split': [2, 5],
              'min_samples_leaf': [1, 2],
              'max_features': [None, 'sqrt', 'log2']}

# Create a GridSearchCV object
grid_search = GridSearchCV(dt_cv, param_grid, cv=5, n_jobs=-1)

# Fit the model on the training set
grid_search.fit(X_train, y_train)

# Print the best hyperparameters
print('Best Hyperparameters:', grid_search.best_params_)

# Predict on the testing set using the best model
y_pred_dt = grid_search.predict(X_test)

# Calculate the accuracy score
accuracy = accuracy_score(y_test, y_pred_dt)
print('Accuracy:', accuracy)

"""Fitting and predicting the model"""

dt_cv.fit(X_test, y_test)
# evaluate fine-tuned model on test set
test_accuracy = dt_cv.score(X_test, y_test)
print('Accuracy of decision tree classifier on test set: {:.2f}'.format(test_accuracy))

dt_cv.fit(X_train, y_train)
test_accuracy = dt_cv.score(X_train, y_train)
print('Accuracy of logistic regression classifier on training set: {:.2f}'.format(test_accuracy))

"""hybrid code"""

y_pred_dt = dt_cv.predict(X_test)
y_pred_lr = logreg_cv.predict(X_test)
# Combine predictions using a weighted average
ensemble_pred = (y_pred_dt + y_pred_lr) / 2

# Round the ensemble predictions to obtain the final labels
ensemble_pred = np.round(ensemble_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, ensemble_pred)
print("Test Accuracy:", accuracy)

"""Evaluate the performance of Decision Tree"""

from sklearn.metrics import classification_report, confusion_matrix
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score
from sklearn.metrics import accuracy_score
y_pred_dt = y_pred_dt.astype(int)

print("Accuracy: ", accuracy_score(y_test, y_pred_dt))
print("Precision: ", precision_score(y_test, y_pred_dt, average='weighted'))
print("Recall: ", recall_score(y_test, y_pred_dt, average='weighted'))
print("F1 score: ", f1_score(y_test, y_pred_dt, average='weighted'))
print('Classification Report:\n', classification_report(y_test, y_pred_dt))
cm = confusion_matrix(y_test, y_pred_dt)
print('Confusion Matrix:\n', plot_confusion_matrix(cm, classes=['Fake','True']))

"""plot the Decision Tree"""

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# Plot the decision tree
plt.figure(figsize=(10,10))
plot_tree(dt_cv, filled=True)
plt.show()

"""ROC-AUC Curve of Decision Tree"""

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve,auc
# Predict probabilities for the testing set
y_prob = dt_cv.predict_proba(X_test)[:, 1]

# Calculate the ROC curve and AUC
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve and display the AUC in the plot
plt.plot(fpr, tpr, color='darkorange', label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC-AUC Curve of Decision Tree')
plt.legend(loc="lower right")
plt.show()

"""Bi LSTM with hyperparameter tuning"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import RandomizedSearchCV


# Define the BiLSTM model
X_train = X_train[:, :25]
def create_model(units, dropout_rate):
    model= Sequential()
    model.add(Embedding(input_dim=5000, output_dim=128, input_length=25))
    model.add(Bidirectional(LSTM(units=units, return_sequences=True)))
    model.add(Dropout(dropout_rate))
    model.add(Bidirectional(LSTM(units=units)))
    model.add(Dropout(dropout_rate))
    model.add(Dense(units=1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# Create the KerasClassifier wrapper for scikit-learn compatibility
model = KerasClassifier(build_fn=create_model)

# Define the hyperparameters to tune
param_grid = {
    'units': [32, 64],
    'dropout_rate': [0.2,0.3]
}

# Perform randomized search cross-validation for hyperparameter tuning
random_search = RandomizedSearchCV(model, param_grid, cv=3, scoring='accuracy', n_iter=1)
random_search.fit(X_train, y_train)

# Print the best hyperparameters and the corresponding accuracy
print("Best Hyperparameters: ", random_search.best_params_)
print("Best Accuracy: ", random_search.best_score_)

"""Evaluation of a model"""

# Get the best model from the randomized search
best_model = random_search.best_estimator_
X_test = X_test[:, :25]
# Make predictions on the test data
y_pred_lst = best_model.predict(X_test)

# Calculate the accuracy of the predictions
accuracy = accuracy_score(y_test, y_pred_lst)
print("Test Accuracy:", accuracy)

from sklearn.metrics import classification_report, confusion_matrix
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score
from sklearn.metrics import accuracy_score
y_pred = y_pred.astype(int)

print("Accuracy: ", accuracy_score(y_test, y_pred_lst))
print("Precision: ", precision_score(y_test, y_pred_lst, average='weighted'))
print("Recall: ", recall_score(y_test, y_pred_lst, average='weighted'))
print("F1 score: ", f1_score(y_test, y_pred_lst, average='weighted'))
print('Classification Report:\n', classification_report(y_test, y_pred_lst))
cm = confusion_matrix(y_test, y_pred_lst)
print('Confusion Matrix:\n', plot_confusion_matrix(cm, classes=['Fake','True']))

"""ROC-AUC CURVE"""

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve

logit_roc_auc = roc_auc_score(y_test, random_search.predict(X_test))
fpr, tpr, thresholds = roc_curve(y_test, random_search.predict_proba(X_test)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='Bi lstm (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([-0.01, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.show()